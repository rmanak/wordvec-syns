{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating synonym and not synonym dataset using wordnet\n",
    "\n",
    "First let's create a dataset of pairs of words that are synonyms or not synonyms using wordnet's synsets.\n",
    "\n",
    "*Note*: \"not synonyms\" is not equivalent of antonyms, it simply means selecting any word `w2` in `(w1, w2)` pair in which the word `w2` is not in the set of synonyms to `w1`, i.e. we are simply performing **negative sampling**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import combinations, chain\n",
    "import gzip\n",
    "\n",
    "import networkx as nx \n",
    "from nltk.corpus import wordnet as wn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import editdistance\n",
    "import pandas as pd\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_synsets(part_of_speeches=None, verbose=True):\n",
    "    \"\"\"\n",
    "    returns a dictionary where key is a particular part of speech\n",
    "    and value is the list of all synsets in that POS, if default\n",
    "    `part_of_speeches` is `None`, will use, verb, noun and adjectives.\n",
    "    \"\"\"\n",
    "    if part_of_speeches is None:\n",
    "        part_of_speeches={'verb': 'v', 'noun': 'n', 'adjective': 'a'}\n",
    "        \n",
    "    pos_synsets = dict()\n",
    "    for name, pos in part_of_speeches.items():\n",
    "        pos_synsets[name] = list(wn.all_synsets(pos))\n",
    "        if verbose:\n",
    "            print(f\"found {len(pos_synsets[name])} synsets for {name}\")\n",
    "            \n",
    "    return pos_synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pos_synsets = get_synsets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _is_single_word(word):\n",
    "    \"\"\"\n",
    "    helper function for removing lemma names that contains multiple words, \n",
    "    separated by `-` or `_` \n",
    "    \"\"\"\n",
    "    return (('_' not in word) and ('-' not in word))\n",
    "\n",
    "def get_syngraph_wordset(pos_synsets, verbose=True):\n",
    "    \"\"\"\n",
    "    Uses the `pos_synsets` dictionary to create a dictionary\n",
    "    of same keys where values are the synonym graphs using `.lemma_names()`\n",
    "    of each synsets, see: http://www.nltk.org/howto/wordnet.html\n",
    "    \"\"\"\n",
    "    syn_graphs = dict()\n",
    "    for pos, synsets in pos_synsets.items():\n",
    "        syn_graphs[pos] = nx.Graph()\n",
    "        \n",
    "        for synset in synsets:\n",
    "            lemma_names = [x for x in synset.lemma_names() if _is_single_word(x)]\n",
    "            \n",
    "            if len(lemma_names) > 1:\n",
    "                syn_graphs[pos].add_edges_from(combinations(lemma_names, 2))\n",
    "                        \n",
    "        if verbose:\n",
    "            msg_fmt = \"Found {} synonym pairs and {} unique words in {}\"\n",
    "            print(msg_fmt.format(len(syn_graphs[pos].edges), \n",
    "                                 len(syn_graphs[pos].nodes), \n",
    "                                 pos))\n",
    "    \n",
    "    return syn_graphs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "syn_graphs = get_syngraph_wordset(pos_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(list(syn_graphs['verb'].neighbors('change')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(list(syn_graphs['noun'].neighbors('ocean')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(list(syn_graphs['adjective'].neighbors('large')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_subgraph(graph, subset=['change', 'buy']):\n",
    "    \"\"\"\n",
    "    Create a subgraph of graph with only nodes and nieghbors\n",
    "    given in `subset`.\n",
    "    \"\"\"\n",
    "    nodes = []\n",
    "    \n",
    "    for node in subset:\n",
    "        nodes.append(node)\n",
    "        nodes.extend(graph.neighbors(node))\n",
    "    \n",
    "    subgraph = graph.subgraph(nodes)\n",
    "    \n",
    "    return subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subgraph = get_subgraph(syn_graphs['verb'], subset=['sell', 'buy', 'change'])\n",
    "pos=nx.spring_layout(subgraph, iterations=150, k=1.5)\n",
    "nx.draw(subgraph, pos=pos)\n",
    "nx.draw_networkx_labels(subgraph, pos=pos, font_size=10)\n",
    "plt.show()\n",
    "# plt.savefig('graph.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![alttag](./img/graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(syn_graphs, train_test_component_ratio=0.5, \n",
    "                   negative_sampling_count=1):\n",
    "    weights = {'verb': 1.15,\n",
    "               'adjective': 1.0,\n",
    "               'noun': 1.0}\n",
    "\n",
    "    for pos, graph in syn_graphs.items():\n",
    "        edges = [(x, y) for (x, y) in graph.edges]\n",
    "        all_words_len = len(graph.nodes)\n",
    "        train_len = int(all_words_len*train_test_component_ratio)\n",
    "        train_words = set()\n",
    "        test_words = set()\n",
    "        train_edges = list()\n",
    "        test_edges = list()\n",
    "        for (x, y) in edges:\n",
    "            if len(train_words) < train_len*weights[pos]:\n",
    "                train_edges.append((x, y))\n",
    "                train_words.update({x, y})\n",
    "            else:\n",
    "                test_edges.append((x, y))\n",
    "                test_words.update({x, y})\n",
    "                \n",
    "        print(\"{}, train {}, test {}, intersec {}, train {}, test {}\".format(\n",
    "                pos, len(train_words), len(test_words),\n",
    "                len(train_words.intersection(test_words)),\n",
    "                len(train_words.difference(test_words)), \n",
    "                len(test_words.difference(train_words)))) \n",
    "        \n",
    "        intersection = train_words.intersection(test_words)\n",
    "        train_sampling = train_words.difference(intersection)\n",
    "        test_sampling = test_words.difference(intersection)\n",
    "        for (data, split) in zip([train_edges, test_edges], ['train', 'test']):\n",
    "            for (w1, w2) in data:\n",
    "                if editdistance.eval(w1, w2) < 2:\n",
    "                    continue\n",
    "                if (w1 in intersection) or (w2 in intersection):\n",
    "                    continue\n",
    "                pos_words =[w1]\n",
    "                pos_words.extend(list(graph.neighbors(w1)))\n",
    "                pos_words = set(pos_words)\n",
    "                if split == 'train':\n",
    "                    neg_words = list(train_sampling.difference(pos_words))\n",
    "                else:\n",
    "                    neg_words = list(test_sampling.difference(pos_words))\n",
    "                neg_sampling = random.choices(neg_words, k=negative_sampling_count)\n",
    "                yield {'word1': w1, 'word2': w2, 'synonym': 1, 'pos': pos, 'split': split}\n",
    "                for w3 in neg_sampling:\n",
    "                    yield {'word1': w1, 'word2': w3, 'synonym': 0, 'pos': pos, 'split': split}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(list(create_dataset(syn_graphs, negative_sampling_count=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.groupby(['pos', 'split', 'synonym']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.word1=='change'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../datasets/synonym_dataset.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
